---
title: "classification1"
author: "Helen Yezerets"
date: "February 27, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## apply a kNN classifier to help the car recognize this sign
```{r}
# Load the 'class' package
library(class)

# Create a vector of labels
sign_types <- signs$sign_type

# Identify the next_sign using the knn() function.
#Set the train argument equal to the signs data frame without the first column.
#Set the test argument equal to the data frame next_sign.
#Use the vector of labels you created as the cl argument.
knn(train = signs[-1], test = next_sign, cl = sign_types)

# Examine the structure of the signs dataset
str(signs)
# Count the number of signs of each type
table(signs$sign_type)

# Check r10's average red level by sign type
aggregate(r10 ~ signs$sign_type, data = signs, mean)

# Use kNN to identify the test road signs
#Classify the test_signs data using knn().
#Set train equal to the observations in signs without labels.
#Use test_signs for the test argument, again without labels.

sign_types <- signs$sign_type
signs_pred <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types)

# Create a confusion matrix of the predicted versus actual values
#Use table() to explore the classifier's performance at identifying the three sign types (the confusion matrix).
#Create the vector signs_actual by extracting the labels from test_signs.
signs_actual <- test_signs$sign_type

#Pass the vector of predictions and the vector of actual signs to table() to cross tabulate them.
table(signs_pred, signs_actual)

# Compute the accuracy
mean(signs_pred == signs_actual)

```
 With smaller neighborhoods, kNN can identify more subtle patterns in the data.
##Testing other 'k' values
By default, the knn() function in the class package uses only the single nearest neighbor.

Setting a k parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class.

Compare k values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.

The class package is already loaded in your workspace along with the datasets signs, signs_test, and sign_types. The object signs_actual holds the true values of the signs.
```{r}
# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types)
mean(signs_actual == k_1)

# Modify the above to set k = 7
k_7 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 7)
mean(signs_actual == k_7)

# Set k = 15 and compare to the above
k_15 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 15)
mean(signs_actual == k_15)
```

##Seeing how the neighbors voted
When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated.

For example, knowing more about the voters' confidence in the classification could allow an autonomous vehicle to use caution in the case there is any chance at all that a stop sign is ahead.

```{r}
# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <- knn(signs[-1], test=signs_test[-1],cl=sign_types, prob=TRUE, k=7)

# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")

# Examine the first several predictions
head(sign_pred)

# Examine the proportion of votes for the winning class
head(sign_prob)
```
##Why normalize data?
Before applying kNN to a classification task, it is common practice to rescale the data using a technique like min-max normalization. What is the purpose of this step?

To ensure all data elements may contribute equal shares to distance.

