---
title: "datacamp_TS5_Simple MA_Model"
author: "Helen Yezerets"
date: "February 11, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Simulate the simple moving average model
The simple moving average (MA) model is a parsimonious time series model used to account for very short-run autocorrelation. It does have a regression like form, but here each observation is regressed on the previous innovation, which is not actually observed. Like the autoregressive (AR) model, the MA model includes the white noise (WN) model as special case.

As with previous models, the MA model can be simulated using the **arima.sim()** command by setting the model argument to **list(ma = theta)**, where theta is a slope parameter from the interval **(-1, 1)**. Once again, you also need to specifcy the series length using the n argument.

```{r}
# Generate MA model with slope 0.5
x <- arima.sim(model = list(ma=0.5), n =100)

# Generate MA model with slope 0.9
y <- arima.sim(model = list(ma=0.9), n =100)

# Generate MA model with slope -0.5
z <- arima.sim(model = list(ma=-0.5), n =100)

# Plot all three models together
plot.ts(cbind(x, y, z))

```
###Analysis:
there is some very short-run persistence for the positive slope values (x and y), and the series has a tendency to alternate when the slope value is negative (z).

##Estimate the autocorrelation function (ACF) for a moving average
Now that you've simulated some MA data using the arima.sim() command.

```{r}
# Calculate ACF for x
acf(x)

# Calculate ACF for y

acf(y)
# Calculate ACF for z
acf(z)

```
###Analysis:
- The series x has positive sample autocorrelation at the first lag, but it is approximately zero at other lags. 
- The series y has a larger sample autocorrelation at its first lag, but it is also approximately zero for the others. 
- The series z has an alternating pattern, and its sample autocorrelation is negative at the first lag. However, similar to the others, it is approximately zero for all higher lags.

##Estimate a simple moving average model
Now that you've simulated some MA models and calculated the ACF from these models, your next step is to fit the simple moving average (MA) model to some data using the **arima()** command. For a given time series x we can fit the simple moving average (MA) model using **arima(..., order = c(0, 0, 1))**. Note for reference that an **MA model is an ARIMA(0, 0, 1) model**.

```{r Nile}

# Fit the MA model to x
MA<-arima(x, order = c(0,0,1))
print(MA)

# Paste the slope (ma1) estimate below
0.7928

# Paste the slope mean (intercept) estimate below
0.1589

# Paste the innovation variance (sigma^2) estimate below
0.9576

# Fit the MA model to Nile
MA <- arima(Nile, order = c(0,0,1))
print(MA)

# Plot Nile and MA_fit 
ts.plot(Nile)
MA_fit <- Nile - resid(MA)
points(MA_fit, type = "l", col = 2, lty = 2)
```
###Analysis:
By fitting an MA model to your Nile data, you're able to capture variation in the data for future prediction. Based on the plot you've generated, does the MA model appear to be a strong fit for the Nile data?

##Simple forecasts from an estimated MA model
Now that you've estimated a MA model with your Nile data, the next step is to do some simple forecasting with your model. As with other types of models, you can use the **predict()** function to make simple forecasts from your estimated MA model. Recall that the **$pred value is the forecast**, while the **$se value is a standard error for that forecast**, each of which is based on the fitted MA model.

Once again, to make predictions for several periods beyond the last observation you can use the **n.ahead = h** argument in your call to predict(). The forecasts are made recursively from 1 to h-steps ahead from the end of the observed time series. However, note that except for the 1-step forecast, all forecasts from the MA model are equal to the estimated mean (intercept).

```{r}
# Make a 1-step forecast based on MA
predict_MA <-predict(MA)

# Obtain the 1-step forecast using $pred[1]
predict_MA$pred[1]

# Make a 1-step through 10-step forecast based on MA
predict(MA, n.ahead=10)

# Plot the Nile series plus the forecast and 95% prediction intervals
ts.plot(Nile, xlim = c(1871, 1980))
MA_forecasts <- predict(MA, n.ahead = 10)$pred
MA_forecast_se <- predict(MA, n.ahead = 10)$se
points(MA_forecasts, type = "l", col = 2)
points(MA_forecasts - 2*MA_forecast_se, type = "l", col = 2, lty = 2)
points(MA_forecasts + 2*MA_forecast_se, type = "l", col = 2, lty = 2)
```

##AR vs MA models
As you've seen, autoregressive (AR) and simple moving average (MA) are two useful approaches to modeling time series. But how can you determine whether an AR or MA model is more appropriate in practice?

To determine model fit, you can measure the Akaike information criterion (AIC) and Bayesian information criterion (BIC) for each model. While the math underlying the AIC and BIC is beyond the scope of this course, for your purposes the main idea is these these indicators penalize models with more estimated parameters, to avoid overfitting, and smaller values are preferred. All factors being equal, a model that produces a lower AIC or BIC than another model is considered a better fit.

To estimate these indicators, you can use the AIC() and BIC() commands, both of which require a single argument to specify the model in question.

```{r}
# Find correlation between AR_fit and MA_fit
cor(AR_fit, MA_fit)

# Find AIC of AR
AIC(AR)

# Find AIC of MA
AIC(MA)

# Find BIC of AR
BIC(AR)

# Find BIC of MA
BIC(MA)
```

Although the predictions from both models are very similar (indeed, they have a correlation coeffiicent of 0.94), both the AIC and BIC indicate that the AR model is a slightly better fit for your Nile data.


Series A shows short-run dependence but reverts quickly to the mean, so it must be the MA model. Series B and C are consistent with AR and RW, respectively. Series D does not show any clear patterns, so it must be the WN model.
